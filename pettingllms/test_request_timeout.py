#!/usr/bin/env python3
"""
æµ‹è¯•è„šæœ¬ï¼šéªŒè¯AsyncLLMServerManagerçš„è¯·æ±‚è¶…æ—¶æ¸…ç†æœºåˆ¶
"""

import asyncio
import time
from datetime import datetime, timedelta
from unittest.mock import Mock, AsyncMock
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from pettingllms.trainer.utils import AsyncLLMServerManager, RequestState
from omegaconf import DictConfig
import ray


class MockServer:
    """æ¨¡æ‹ŸvLLMæœåŠ¡å™¨"""
    def __init__(self):
        self.requests = {}
        self.abort_called = []
    
    async def generate(self, prompt_ids, sampling_params, request_id):
        """æ¨¡æ‹Ÿç”Ÿæˆå“åº”"""
        self.requests[request_id] = {
            'prompt_ids': prompt_ids,
            'sampling_params': sampling_params,
            'timestamp': datetime.now()
        }
        
        # æ¨¡æ‹Ÿç”Ÿæˆä¸€äº›token ids
        await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return [1, 2, 3, 4, 5]  # æ¨¡æ‹Ÿå“åº”token
    
    async def abort_request(self, request_id):
        """æ¨¡æ‹Ÿä¸­æ­¢è¯·æ±‚"""
        self.abort_called.append(request_id)
        if request_id in self.requests:
            del self.requests[request_id]
        print(f"âœ… æ¨¡æ‹ŸæœåŠ¡å™¨å·²ä¸­æ­¢è¯·æ±‚: {request_id}")


async def test_request_timeout_mechanism():
    """æµ‹è¯•è¯·æ±‚è¶…æ—¶æœºåˆ¶"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•è¯·æ±‚è¶…æ—¶æ¸…ç†æœºåˆ¶...")
    
    # åˆ›å»ºæ¨¡æ‹Ÿé…ç½®å’ŒæœåŠ¡å™¨
    config = DictConfig({
        "actor_rollout_ref": {
            "rollout": {
                "prompt_length": 512,
                "response_length": 256
            }
        }
    })
    
    # åˆ›å»ºæ¨¡æ‹ŸæœåŠ¡å™¨
    mock_server = MockServer()
    
    # åˆ›å»ºAsyncLLMServerManagerï¼Œè®¾ç½®3ç§’è¶…æ—¶
    manager = AsyncLLMServerManager(
        config=config,
        server_handles=[mock_server],
        request_timeout_seconds=3.0  # 3ç§’è¶…æ—¶ç”¨äºå¿«é€Ÿæµ‹è¯•
    )
    
    print(f"ğŸ“Š ç®¡ç†å™¨åˆ›å»ºå®Œæˆï¼Œè¶…æ—¶è®¾ç½®: {manager.request_timeout_seconds}ç§’")
    
    try:
        # æ¨¡æ‹Ÿå‘é€è¯·æ±‚
        from verl.protocol import DataProto
        from transformers import AutoTokenizer
        import torch
        
        # åˆ›å»ºæ¨¡æ‹Ÿtokenizer
        tokenizer = Mock()
        tokenizer.decode = Mock(return_value="æµ‹è¯•å“åº”")
        
        # åˆ›å»ºæ¨¡æ‹ŸDataProto
        mock_dpr = Mock()
        mock_dpr.batch = {
            'input_ids': torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])
        }
        
        print("ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
        
        # å‘é€è¯·æ±‚ä½†ä¸ç«‹å³è·å–å“åº”ï¼Œæ¨¡æ‹Ÿ"æ·»åŠ è¯·æ±‚åä¸è·å–å“åº”"çš„æƒ…å†µ
        response_dpr, response_str = await manager.generate(
            dpr_prompt=mock_dpr,
            tokenizer=tokenizer,
            application_id="test_app",
            rollout_idx=0,
            policy_name="test_policy"
        )
        
        print(f"âœ… è¯·æ±‚å·²å‘é€ï¼Œå½“å‰æ´»è·ƒè¯·æ±‚æ•°: {manager.get_active_requests_count()}")
        
        # ç­‰å¾…è¶…æ—¶æ—¶é—´ + ä¸€äº›ç¼“å†²æ—¶é—´
        print("â° ç­‰å¾…è¶…æ—¶æ¸…ç†æœºåˆ¶è§¦å‘...")
        await asyncio.sleep(8)  # ç­‰å¾…è¶…è¿‡3ç§’è¶…æ—¶ + 5ç§’æ¸…ç†å‘¨æœŸ
        
        # æ£€æŸ¥æ¸…ç†ç»“æœ
        active_count = manager.get_active_requests_count()
        pending_count = manager.get_pending_cleanup_count()
        
        print(f"ğŸ“Š è¶…æ—¶åçŠ¶æ€:")
        print(f"   - æ´»è·ƒè¯·æ±‚æ•°: {active_count}")
        print(f"   - ç­‰å¾…æ¸…ç†æ•°: {pending_count}")
        print(f"   - æœåŠ¡å™¨ä¸­æ­¢è°ƒç”¨æ¬¡æ•°: {len(mock_server.abort_called)}")
        
        if active_count == 0:
            print("âœ… è¶…æ—¶æ¸…ç†æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼è¯·æ±‚å·²è¢«è‡ªåŠ¨æ¸…ç†")
        else:
            print("âŒ è¶…æ—¶æ¸…ç†æœºåˆ¶å¯èƒ½å­˜åœ¨é—®é¢˜")
            
        # æµ‹è¯•æ‰‹åŠ¨æ¸…ç†
        print("\nğŸ§¹ æµ‹è¯•æ‰‹åŠ¨æ¸…ç†æœºåˆ¶...")
        
        # å†å‘é€ä¸€ä¸ªè¯·æ±‚
        response_dpr2, response_str2 = await manager.generate(
            dpr_prompt=mock_dpr,
            tokenizer=tokenizer,
            application_id="test_app_2",
            rollout_idx=1,
            policy_name="test_policy"
        )
        
        print(f"ğŸ“¤ å‘é€ç¬¬äºŒä¸ªè¯·æ±‚ï¼Œæ´»è·ƒè¯·æ±‚æ•°: {manager.get_active_requests_count()}")
        
        # æ‰‹åŠ¨æ¸…ç†è¿™ä¸ªè¯·æ±‚
        request_ids = list(manager.active_requests.keys())
        if request_ids:
            manager.manually_cleanup_request(request_ids[0])
            print(f"ğŸ§¹ æ‰‹åŠ¨æ¸…ç†è¯·æ±‚: {request_ids[0]}")
            print(f"ğŸ“Š æ¸…ç†åæ´»è·ƒè¯·æ±‚æ•°: {manager.get_active_requests_count()}")
        
    finally:
        # åœæ­¢æ¸…ç†ä»»åŠ¡
        manager.stop_cleanup_task()
        print("ğŸ›‘ å·²åœæ­¢æ¸…ç†ä»»åŠ¡")


async def test_multiple_requests_timeout():
    """æµ‹è¯•å¤šä¸ªè¯·æ±‚çš„è¶…æ—¶å¤„ç†"""
    print("\nğŸ”„ æµ‹è¯•å¤šä¸ªè¯·æ±‚çš„è¶…æ—¶å¤„ç†...")
    
    config = DictConfig({
        "actor_rollout_ref": {
            "rollout": {
                "prompt_length": 512,
                "response_length": 256
            }
        }
    })
    
    mock_server = MockServer()
    manager = AsyncLLMServerManager(
        config=config,
        server_handles=[mock_server],
        request_timeout_seconds=2.0  # 2ç§’è¶…æ—¶
    )
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
        from transformers import AutoTokenizer
        import torch
        
        tokenizer = Mock()
        tokenizer.decode = Mock(return_value="æµ‹è¯•å“åº”")
        
        mock_dpr = Mock()
        mock_dpr.batch = {
            'input_ids': torch.tensor([[1, 2, 3, 4, 5]])
        }
        
        # å¿«é€Ÿå‘é€å¤šä¸ªè¯·æ±‚
        print("ğŸ“¤ å‘é€5ä¸ªæµ‹è¯•è¯·æ±‚...")
        tasks = []
        for i in range(5):
            task = asyncio.create_task(
                manager.generate(
                    dpr_prompt=mock_dpr,
                    tokenizer=tokenizer,
                    application_id=f"multi_test_app_{i}",
                    rollout_idx=i,
                    policy_name="test_policy"
                )
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        await asyncio.gather(*tasks)
        
        print(f"âœ… 5ä¸ªè¯·æ±‚å·²å‘é€ï¼Œæ´»è·ƒè¯·æ±‚æ•°: {manager.get_active_requests_count()}")
        
        # ç­‰å¾…è¶…æ—¶æ¸…ç†
        print("â° ç­‰å¾…æ‰¹é‡è¶…æ—¶æ¸…ç†...")
        await asyncio.sleep(7)  # ç­‰å¾…è¶…è¿‡2ç§’è¶…æ—¶ + 5ç§’æ¸…ç†å‘¨æœŸ
        
        print(f"ğŸ“Š æ‰¹é‡æ¸…ç†åæ´»è·ƒè¯·æ±‚æ•°: {manager.get_active_requests_count()}")
        print(f"ğŸ“Š æœåŠ¡å™¨ä¸­æ­¢è°ƒç”¨æ¬¡æ•°: {len(mock_server.abort_called)}")
        
        if manager.get_active_requests_count() == 0:
            print("âœ… æ‰¹é‡è¶…æ—¶æ¸…ç†æœºåˆ¶å·¥ä½œæ­£å¸¸ï¼")
        else:
            print("âŒ æ‰¹é‡è¶…æ—¶æ¸…ç†å¯èƒ½å­˜åœ¨é—®é¢˜")
            
    finally:
        manager.stop_cleanup_task()


if __name__ == "__main__":
    print("ğŸ§ª AsyncLLMServerManager è¯·æ±‚è¶…æ—¶æœºåˆ¶æµ‹è¯•")
    print("=" * 50)
    
    # ç”±äºæˆ‘ä»¬æ²¡æœ‰çœŸå®çš„Rayç¯å¢ƒï¼Œè¿™é‡Œåªèƒ½åšæ¨¡æ‹Ÿæµ‹è¯•
    # å®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„Rayå’ŒvLLMç¯å¢ƒ
    
    try:
        # æµ‹è¯•å•ä¸ªè¯·æ±‚è¶…æ—¶
        asyncio.run(test_request_timeout_mechanism())
        
        # æµ‹è¯•å¤šä¸ªè¯·æ±‚è¶…æ—¶
        asyncio.run(test_multiple_requests_timeout())
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
